# LLaDA Training Configuration

# Model configuration
model:
  pretrained_model_name: "GSAI-ML/LLaDA-8B-Instruct"
  hidden_size: 4096
  context_length: 1024
  pos_weight: 0.277  # For classification model

# Data configuration
data:
  embedding_dir: "./embedded_datasets"
  num_workers: 1
  val_test_perc: 0.05
  cache_dir: "/root/cache_hidden"

# Training parameters
training:
  batch_size: 4
  learning_rate: 1e-3
  epochs: 1
  accumulate_grad: 1
  max_steps: 5000
  seed: 42
  val_check_interval: 200 # Check validation every 200 steps

# Validation parameters
validation:
  val_check_steps: 100

# Checkpoint parameters
checkpoint:
  dir: "./checkpoints"
  steps: 200
  resume_from: null  # Set to path to resume from checkpoint

# Early stopping parameters
early_stopping:
  patience: 3
  min_delta: 0.001
  monitor: "val/loss"
  mode: "min"

# Logging and monitoring
logging:
  run_name: "variable-length-llada"
  project_name: "diffusion-llms"
  log_dir: "./logs"

# Model-specific configurations
model_to_train:
  type: "regressor"  # Options: "classifier", "regressor", "full_regressor"
  output_dir: "./models/regressor_mlp"
  pos_weight: 0.277
  
  #regressor:
  #  output_dir: "./models/regressor"
  #
  #full_regressor:
  #  output_dir: "./models/full_regressor"