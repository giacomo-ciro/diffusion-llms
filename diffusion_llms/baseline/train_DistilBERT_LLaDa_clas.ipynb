{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c8d6cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lgand\\anaconda3\\envs\\DiffuGPT\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from datamodule import PromptDataset, get_length\n",
    "from model_baseline import DistilBertClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84e63b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# === Tokenizer ===\n",
    "tokenizer_BERT = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "tokenizer_LLaDa = AutoTokenizer.from_pretrained('GSAI-ML/LLaDA-8B-Instruct', trust_remote_code=True)\n",
    "steps = [32, 64, 128, 256, 512, 1024]\n",
    "# I would have done with also 2048 and 4096 but in the training data there are no examples with that length\n",
    "\n",
    "# === Load data ===\n",
    "df_train = pd.read_csv(r\"..\\data\\train.csv\")\n",
    "train_data = list(zip(df_train[\"user_prompt\"], get_length(df_train[\"model_response\"], tokenizer_LLaDa, max_length=1024, steps= steps)))\n",
    "del df_train\n",
    "\n",
    "df_test = pd.read_csv(r\"..\\data\\test.csv\")\n",
    "df_test = df_test.dropna(subset=[\"model_response\"])\n",
    "data_test = list(zip(df_test[\"user_prompt\"], get_length(df_test[\"model_response\"], tokenizer_LLaDa, max_length= 1024, steps= steps)))\n",
    "del df_test\n",
    "\n",
    "val_data, test_data = train_test_split(data_test, test_size=0.3, random_state=42)\n",
    "\n",
    "# All the training prompt except one have length < 64\n",
    "train_ds = PromptDataset(train_data, tokenizer_BERT, max_len=64)\n",
    "val_ds = PromptDataset(val_data, tokenizer_BERT, max_len=128)\n",
    "test_ds = PromptDataset(test_data, tokenizer_BERT, max_len=128)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=16)\n",
    "test_dl = DataLoader(test_ds, batch_size=16)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b92b673",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lgand\\anaconda3\\envs\\DL\\Lib\\site-packages\\huggingface_hub\\file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = DistilBertClassifier(n_classes=6)\n",
    "# model = torch.compile(model).to(device)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0bce4cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(eval_iters = 10):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        if split == 'train':\n",
    "            dataloader = train_dl\n",
    "        else:\n",
    "            dataloader = val_dl\n",
    "        k = 0\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            logits, loss = model(input_ids, attention_mask, labels)\n",
    "            assert loss is not None, \"Loss should not be None\"\n",
    "            losses[k] = loss.item()\n",
    "            k += 1\n",
    "            if k >= eval_iters:\n",
    "                break\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea88e51",
   "metadata": {},
   "source": [
    "### Freezing the DistilBERT parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25b73e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.classifier.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b26c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_interval = 200\n",
    "max_iters = len(train_dl)\n",
    "\n",
    "# === Training loop ===\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for i, batch in enumerate(train_dl):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        logits, loss = model(input_ids, attention_mask, labels)\n",
    "        assert loss is not None, \"Loss should not be None\"\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        if i % eval_interval == 0 or i == max_iters - 1:\n",
    "                losses = estimate_loss()\n",
    "                print(f\"step {i}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    print(f\"[Epoch {epoch+1}] Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cc092c",
   "metadata": {},
   "source": [
    "### Fine tuning: changing also the parameters of DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b4386c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Congela tutti i parametri di BERT\n",
    "for param in model.encoder.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c2700d",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_interval = 200\n",
    "max_iters = len(train_dl)\n",
    "\n",
    "# === Training loop ===\n",
    "for epoch in range(6):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for i, batch in enumerate(train_dl):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        logits, loss = model(input_ids, attention_mask, labels)\n",
    "        assert loss is not None, \"Loss should not be None\"\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        if i % eval_interval == 0 or i == max_iters - 1:\n",
    "                losses = estimate_loss()\n",
    "                print(f\"step {i}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    print(f\"[Epoch {epoch+1}] Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6b707148",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"checkpoints/DistilBERT_LLaDa_1.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb70fb9",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b97ed5b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DistilBertClassifier(n_classes=6)\n",
    "model.load_state_dict(torch.load(\"checkpoints/DistilBERT_LLaDa_1.pth\"))\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05c8187b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[-2.1020, -0.9028,  1.1409,  2.2895,  2.6583, -3.0865]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>), None)\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Can you explain the theory of relativity?\"\n",
    "input_enc = tokenizer_BERT(input_text, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=512).to(device)\n",
    "output = model(input_enc['input_ids'], input_enc['attention_mask'])\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0b2c15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[ 2.9584,  2.9615,  1.7493, -0.4979, -3.9948, -4.0273]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>), None)\n"
     ]
    }
   ],
   "source": [
    "input_text = \"What's your name?\"\n",
    "input_enc = tokenizer_BERT(input_text, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=512).to(device)\n",
    "output = model(input_enc['input_ids'], input_enc['attention_mask'])\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1e215d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[ 1.6350,  0.8858,  1.1286,  0.3247, -1.9087, -2.3112]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>), None)\n"
     ]
    }
   ],
   "source": [
    "input_text = \"What is 3 + 3?\"\n",
    "input_enc = tokenizer_BERT(input_text, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=512).to(device)\n",
    "output = model(input_enc['input_ids'], input_enc['attention_mask'])\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b44a252f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def see_prediction(kappa = 1):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['val']:\n",
    "        losses = torch.zeros(kappa)\n",
    "        if split == 'train':\n",
    "            dataloader = train_dl\n",
    "        else:\n",
    "            dataloader = val_dl\n",
    "        k = 0\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            logits, loss = model(input_ids, attention_mask, labels)\n",
    "            pred = torch.argmax(logits, dim=1)\n",
    "            print(f\"pred: {[(pred[i].item(), labels[i].item()) for i in range(16)]}\")\n",
    "            assert loss is not None, \"Loss should not be None\"\n",
    "            losses[k] = loss.item()\n",
    "            k += 1\n",
    "            if k >= kappa:\n",
    "                break\n",
    "        out[split] = losses\n",
    "    model.train()\n",
    "    print(\"mean cross entropy loss: \", out[\"val\"].mean())\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e284c01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: [(2, 4), (3, 1), (2, 2), (3, 3), (2, 1), (4, 4), (3, 2), (2, 3), (4, 3), (4, 3), (4, 4), (4, 4), (4, 4), (0, 2), (3, 3), (4, 4)]\n",
      "pred: [(4, 3), (3, 4), (4, 2), (3, 4), (4, 3), (4, 4), (2, 0), (3, 3), (4, 4), (3, 3), (0, 0), (4, 3), (3, 3), (3, 2), (3, 3), (4, 0)]\n",
      "pred: [(2, 1), (3, 2), (0, 0), (2, 3), (2, 3), (4, 3), (1, 4), (4, 2), (4, 2), (3, 4), (4, 3), (2, 3), (3, 2), (0, 0), (3, 1), (1, 3)]\n",
      "pred: [(0, 1), (2, 1), (4, 4), (4, 4), (3, 2), (4, 3), (4, 4), (3, 4), (4, 4), (4, 4), (2, 0), (0, 1), (4, 2), (4, 3), (4, 4), (0, 0)]\n",
      "pred: [(1, 0), (0, 0), (3, 3), (4, 4), (3, 4), (4, 4), (2, 2), (3, 4), (4, 4), (4, 2), (4, 4), (4, 3), (1, 0), (2, 2), (2, 1), (0, 2)]\n",
      "pred: [(2, 0), (3, 4), (3, 3), (3, 2), (4, 4), (3, 4), (4, 4), (2, 2), (4, 4), (4, 3), (3, 3), (3, 3), (3, 3), (2, 2), (1, 0), (4, 3)]\n",
      "pred: [(4, 0), (3, 2), (4, 4), (4, 2), (3, 2), (4, 3), (4, 4), (4, 4), (2, 0), (4, 3), (3, 3), (2, 2), (3, 4), (2, 3), (3, 3), (4, 4)]\n",
      "pred: [(3, 4), (4, 3), (4, 3), (3, 2), (3, 3), (3, 3), (3, 4), (4, 5), (1, 3), (0, 0), (2, 0), (2, 2), (4, 4), (2, 3), (4, 4), (4, 4)]\n",
      "pred: [(4, 4), (3, 3), (4, 2), (3, 4), (3, 4), (3, 2), (4, 4), (2, 3), (4, 3), (2, 2), (3, 2), (4, 4), (3, 4), (3, 2), (3, 3), (2, 2)]\n",
      "pred: [(4, 4), (4, 4), (2, 2), (4, 3), (4, 3), (3, 3), (4, 4), (4, 3), (4, 4), (4, 4), (2, 2), (2, 0), (4, 3), (4, 4), (4, 4), (3, 1)]\n",
      "mean cross entropy loss:  tensor(1.2957)\n"
     ]
    }
   ],
   "source": [
    "see_prediction(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "719c8a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "def evaluate_accuracy(model, dataloader, device='cpu'):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            logits, loss = model(input_ids, attention_mask, labels)\n",
    "            pred = torch.argmax(logits, dim=1)\n",
    "\n",
    "            all_preds.extend(pred.cpu().tolist())\n",
    "            all_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    mse = mean_squared_error(all_labels, all_preds)\n",
    "    return accuracy, mse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c7b9c5fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 48.26%\n",
      "Mean Squared Error: 1.0589\n"
     ]
    }
   ],
   "source": [
    "acc, mse = evaluate_accuracy(model, val_dl, device=device)\n",
    "print(f\"Accuracy: {acc:.2%}\")\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60abc06f",
   "metadata": {},
   "source": [
    "### Final evaluation: obtain the list of predictions on the test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a6b5c7f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 3)\n",
      "(4998, 3)\n"
     ]
    }
   ],
   "source": [
    "df_test = pd.read_csv(r\"..\\data\\test.csv\")\n",
    "print(df_test.shape)\n",
    "df_test = df_test.dropna(subset=[\"model_response\"])\n",
    "print(df_test.shape)\n",
    "data_test = list(zip(df_test[\"user_prompt\"], get_length(df_test[\"model_response\"], tokenizer_LLaDa, max_length= 1024, steps= steps)))\n",
    "del df_test\n",
    "\n",
    "\n",
    "test_ds = PromptDataset(data_test, tokenizer_BERT, max_len=64)\n",
    "test_dl = DataLoader(test_ds, batch_size=16)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f257c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, dataloader, device='cpu'):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            logits, loss = model(input_ids, attention_mask, labels)\n",
    "            pred = torch.argmax(logits, dim=1)\n",
    "\n",
    "            all_preds.extend(pred.cpu().tolist())\n",
    "            all_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "    return all_preds, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "383959d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds, all_labels = get_predictions(model, test_dl, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5745a6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [32, 64, 128, 256, 512, 1024]\n",
    "all_preds = [steps[pred] for pred in all_preds]\n",
    "all_labels = [steps[label] for label in all_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1fd4cd17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([256, 512, 128, ..., 256, 512, 256]),\n",
       " array([ 32, 128, 256, ..., 128, 512,  32]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array(all_preds), np.array(all_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fb49116f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 29398.1016\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mse = mean_squared_error(all_labels, all_preds)\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5719aef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(\"prediction_test\", exist_ok=True)\n",
    "\n",
    "# Save all_preds as a numpy array\n",
    "np.save(\"prediction_test/DistilBERT_LLaDa_clas.npy\", np.array(all_preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DiffuGPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
