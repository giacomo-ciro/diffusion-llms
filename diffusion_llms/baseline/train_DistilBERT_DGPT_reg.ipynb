{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c8d6cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from datamodule import PromptDataset, get_length_reg\n",
    "from model_baseline import DistilBertRegressor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84e63b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lgand\\anaconda3\\envs\\DL\\Lib\\site-packages\\huggingface_hub\\file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2258 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# === Tokenizer ===\n",
    "tokenizer_BERT = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "tokenizer_GPT2 = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "steps = [32, 64, 128, 256, 512]\n",
    "\n",
    "# === Load data ===\n",
    "df_train = pd.read_csv(r\"..\\data\\train.csv\")\n",
    "train_data = list(zip(df_train[\"user_prompt\"], get_length_reg(df_train[\"model_response\"], tokenizer_GPT2)))\n",
    "del df_train\n",
    "\n",
    "df_test = pd.read_csv(r\"..\\data\\test.csv\")\n",
    "df_test = df_test.dropna(subset=[\"model_response\"])\n",
    "data_test = list(zip(df_test[\"user_prompt\"], get_length_reg(df_test[\"model_response\"], tokenizer_GPT2)))\n",
    "del df_test\n",
    "\n",
    "val_data, test_data = train_test_split(data_test, test_size=0.3, random_state=42)\n",
    "\n",
    "# All the training prompt except one have length < 64\n",
    "train_ds = PromptDataset(train_data, tokenizer_BERT, max_len=64)\n",
    "val_ds = PromptDataset(val_data, tokenizer_BERT, max_len=128)\n",
    "test_ds = PromptDataset(test_data, tokenizer_BERT, max_len=128)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=32)\n",
    "test_dl = DataLoader(test_ds, batch_size=32)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5b0271f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lgand\\anaconda3\\envs\\DL\\Lib\\site-packages\\huggingface_hub\\file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = DistilBertRegressor()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bce4cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(eval_iters = 10):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        if split == 'train':\n",
    "            dataloader = train_dl\n",
    "        else:\n",
    "            dataloader = val_dl\n",
    "        k = 0\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            logits, loss = model(input_ids, attention_mask, labels)\n",
    "            assert loss is not None, \"Loss should not be None\"\n",
    "            losses[k] = loss.item()\n",
    "            k += 1\n",
    "            if k >= eval_iters:\n",
    "                break\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea88e51",
   "metadata": {},
   "source": [
    "### Freezing the DistilBERT parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25b73e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.regressor.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57b26c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 58442.3633, val loss 60240.4141\n",
      "step 200: train loss 59391.0703, val loss 59417.0625\n",
      "step 400: train loss 58902.9922, val loss 58486.9883\n",
      "step 445: train loss 57124.1562, val loss 58249.7500\n",
      "[Epoch 1] Loss: 26795071.7773\n",
      "step 0: train loss 59229.4297, val loss 58244.4688\n",
      "step 200: train loss 53898.8203, val loss 57062.4180\n",
      "step 400: train loss 54039.8984, val loss 55720.2500\n",
      "step 445: train loss 57511.3945, val loss 55401.7734\n",
      "[Epoch 2] Loss: 25719094.5078\n",
      "step 0: train loss 55070.4492, val loss 55394.3672\n",
      "step 200: train loss 59326.0742, val loss 53834.3125\n",
      "step 400: train loss 72129.3594, val loss 52162.4805\n",
      "step 445: train loss 50230.7109, val loss 51759.8750\n",
      "[Epoch 3] Loss: 24264466.9238\n",
      "step 0: train loss 44094.7461, val loss 51750.6758\n",
      "step 200: train loss 49264.7305, val loss 49945.2148\n",
      "step 400: train loss 44973.9414, val loss 48069.3008\n",
      "step 445: train loss 55843.3047, val loss 47624.8359\n",
      "[Epoch 4] Loss: 22440830.6426\n",
      "step 0: train loss 44991.9727, val loss 47614.8125\n",
      "step 200: train loss 46541.2188, val loss 45689.2891\n",
      "step 400: train loss 39408.2188, val loss 43732.4688\n",
      "step 445: train loss 60965.8516, val loss 43284.1992\n",
      "[Epoch 5] Loss: 20506265.6660\n",
      "step 0: train loss 37714.8984, val loss 43273.6914\n",
      "step 200: train loss 44120.7695, val loss 41309.0781\n",
      "step 400: train loss 40134.7578, val loss 39367.7852\n",
      "step 445: train loss 40010.3008, val loss 38951.5195\n",
      "[Epoch 6] Loss: 18471958.3359\n",
      "step 0: train loss 41219.6953, val loss 38942.3359\n",
      "step 200: train loss 40170.2188, val loss 37081.3398\n",
      "step 400: train loss 35432.1797, val loss 35246.0977\n",
      "step 445: train loss 30206.0898, val loss 34842.0391\n",
      "[Epoch 7] Loss: 16505403.8750\n",
      "step 0: train loss 30747.5195, val loss 34832.8242\n",
      "step 200: train loss 32351.5195, val loss 33074.7578\n",
      "step 400: train loss 30490.9883, val loss 31378.3496\n",
      "step 445: train loss 25554.0156, val loss 31012.2500\n",
      "[Epoch 8] Loss: 14617271.6787\n",
      "step 0: train loss 30686.2598, val loss 31004.7129\n",
      "step 200: train loss 26256.0820, val loss 29453.3223\n",
      "step 400: train loss 23719.6250, val loss 28044.2656\n",
      "step 445: train loss 25487.6133, val loss 27730.0215\n",
      "[Epoch 9] Loss: 12917424.7148\n",
      "step 0: train loss 26237.6465, val loss 27723.5254\n",
      "step 200: train loss 26685.0840, val loss 26425.3223\n",
      "step 400: train loss 24374.1348, val loss 25279.3691\n",
      "step 445: train loss 22599.6914, val loss 25040.2305\n",
      "[Epoch 10] Loss: 11459830.8682\n"
     ]
    }
   ],
   "source": [
    "eval_interval = 200\n",
    "max_iters = len(train_dl)\n",
    "\n",
    "# === Training loop ===\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for i, batch in enumerate(train_dl):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        logits, loss = model(input_ids, attention_mask, labels)\n",
    "        assert loss is not None, \"Loss should not be None\"\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        if i % eval_interval == 0 or i == max_iters - 1:\n",
    "                losses = estimate_loss()\n",
    "                print(f\"step {i}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    print(f\"[Epoch {epoch+1}] Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed3bcd88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 42148.8086, val loss 25034.8301\n",
      "step 200: train loss 23393.8242, val loss 24014.9648\n",
      "step 400: train loss 22812.0625, val loss 23148.7617\n",
      "step 445: train loss 21875.8711, val loss 22972.7422\n",
      "[Epoch 1] Loss: 10232775.1543\n",
      "step 0: train loss 20114.6992, val loss 22968.9180\n",
      "step 200: train loss 19020.5820, val loss 22227.3477\n",
      "step 400: train loss 19536.5352, val loss 21609.3281\n",
      "step 445: train loss 18065.9258, val loss 21507.3184\n",
      "[Epoch 2] Loss: 9306091.7202\n",
      "step 0: train loss 17381.3652, val loss 21504.8770\n",
      "step 200: train loss 18614.4707, val loss 21013.4336\n",
      "step 400: train loss 16912.7148, val loss 20663.8125\n",
      "step 445: train loss 18904.1133, val loss 20589.4883\n",
      "[Epoch 3] Loss: 8612875.9502\n",
      "step 0: train loss 19518.0117, val loss 20587.9844\n",
      "step 200: train loss 17718.7891, val loss 20322.0762\n",
      "step 400: train loss 16243.4248, val loss 20127.0098\n",
      "step 445: train loss 17747.9766, val loss 20092.1680\n",
      "[Epoch 4] Loss: 8183896.3242\n",
      "step 0: train loss 15621.2051, val loss 20091.1504\n",
      "step 200: train loss 14721.8877, val loss 19962.7969\n",
      "step 400: train loss 17044.6582, val loss 19889.8906\n",
      "step 445: train loss 16946.0742, val loss 19878.1152\n",
      "[Epoch 5] Loss: 7903222.4575\n"
     ]
    }
   ],
   "source": [
    "eval_interval = 200\n",
    "max_iters = len(train_dl)\n",
    "\n",
    "# === Training loop ===\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for i, batch in enumerate(train_dl):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        logits, loss = model(input_ids, attention_mask, labels)\n",
    "        assert loss is not None, \"Loss should not be None\"\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        if i % eval_interval == 0 or i == max_iters - 1:\n",
    "                losses = estimate_loss()\n",
    "                print(f\"step {i}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    print(f\"[Epoch {epoch+1}] Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cc092c",
   "metadata": {},
   "source": [
    "### Fine tuning: changing also the parameters of DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4386c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Congela tutti i parametri di BERT\n",
    "for param in model.encoder.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98c2700d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 14524.5029, val loss 19836.0664\n",
      "step 200: train loss 10631.6387, val loss 15585.8418\n",
      "step 400: train loss 13101.1016, val loss 14772.9277\n",
      "step 445: train loss 12384.4404, val loss 14201.4404\n",
      "[Epoch 1] Loss: 6293407.0283\n",
      "step 0: train loss 11882.6992, val loss 14186.6846\n",
      "step 200: train loss 10996.8926, val loss 13713.1484\n",
      "step 400: train loss 10839.2715, val loss 14229.2861\n",
      "step 445: train loss 9426.0381, val loss 13533.6191\n",
      "[Epoch 2] Loss: 5425324.1470\n",
      "step 0: train loss 9032.8008, val loss 13543.1904\n",
      "step 200: train loss 10041.5566, val loss 13698.6191\n",
      "step 400: train loss 9306.3955, val loss 13121.5215\n",
      "step 445: train loss 10005.4297, val loss 13029.6270\n",
      "[Epoch 3] Loss: 5090905.2915\n",
      "step 0: train loss 8356.2285, val loss 13007.3262\n",
      "step 200: train loss 20821.6719, val loss 13144.1436\n",
      "step 400: train loss 8848.1729, val loss 12586.4395\n",
      "step 445: train loss 8791.7285, val loss 12603.7168\n",
      "[Epoch 4] Loss: 4819823.4707\n",
      "step 0: train loss 8986.2627, val loss 12635.4404\n",
      "step 200: train loss 8767.6035, val loss 12526.4131\n",
      "step 400: train loss 10970.0137, val loss 12912.1270\n",
      "step 445: train loss 9208.3057, val loss 12354.2666\n",
      "[Epoch 5] Loss: 4617178.6072\n",
      "step 0: train loss 9561.5996, val loss 12418.2090\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 18\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m eval_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m i \u001b[38;5;241m==\u001b[39m max_iters \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m     20\u001b[0m         losses \u001b[38;5;241m=\u001b[39m estimate_loss()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "eval_interval = 200\n",
    "max_iters = len(train_dl)\n",
    "\n",
    "# === Training loop ===\n",
    "for epoch in range(8):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for i, batch in enumerate(train_dl):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        logits, loss = model(input_ids, attention_mask, labels)\n",
    "        assert loss is not None, \"Loss should not be None\"\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        if i % eval_interval == 0 or i == max_iters - 1:\n",
    "                losses = estimate_loss()\n",
    "                print(f\"step {i}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    print(f\"[Epoch {epoch+1}] Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b707148",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"checkpoints/DistilBERT_DGPT_reg.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb70fb9",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b97ed5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lgand\\anaconda3\\envs\\DL\\Lib\\site-packages\\huggingface_hub\\file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "C:\\Users\\lgand\\AppData\\Local\\Temp\\ipykernel_15740\\817096490.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"checkpoints/DistilBERT_DGPT_reg.pth\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DistilBertRegressor()\n",
    "model.load_state_dict(torch.load(\"checkpoints/DistilBERT_DGPT_reg.pth\"))\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05c8187b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([230.7434], device='cuda:0', grad_fn=<SqueezeBackward1>), None)\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Can you explain the theory of relativity?\"\n",
    "input_enc = tokenizer_BERT(input_text, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=512).to(device)\n",
    "output = model(input_enc['input_ids'], input_enc['attention_mask'])\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0b2c15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([56.2916], device='cuda:0', grad_fn=<SqueezeBackward1>), None)\n"
     ]
    }
   ],
   "source": [
    "input_text = \"What's your name?\"\n",
    "input_enc = tokenizer_BERT(input_text, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=512).to(device)\n",
    "output = model(input_enc['input_ids'], input_enc['attention_mask'])\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1e215d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([103.5006], device='cuda:0', grad_fn=<SqueezeBackward1>), None)\n"
     ]
    }
   ],
   "source": [
    "input_text = \"What is 3 + 3?\"\n",
    "input_enc = tokenizer_BERT(input_text, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=512).to(device)\n",
    "output = model(input_enc['input_ids'], input_enc['attention_mask'])\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b44a252f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def see_prediction(kappa = 1):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['val']:\n",
    "        losses = torch.zeros(kappa)\n",
    "        if split == 'train':\n",
    "            dataloader = train_dl\n",
    "        else:\n",
    "            dataloader = val_dl\n",
    "        k = 0\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            logits, loss = model(input_ids, attention_mask, labels)\n",
    "            pred = logits\n",
    "            print(f\"pred: {[(pred[i].item(), labels[i].item()) for i in range(32)]}\")\n",
    "            assert loss is not None, \"Loss should not be None\"\n",
    "            losses[k] = loss.item()\n",
    "            k += 1\n",
    "            if k >= kappa:\n",
    "                break\n",
    "        out[split] = losses\n",
    "    model.train()\n",
    "    print(\"mean cross entropy loss: \", out[\"val\"].mean())\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e284c01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: [(80.5232162475586, 272), (133.73294067382812, 47), (88.93971252441406, 67), (154.083251953125, 159), (91.1819839477539, 53), (317.1280517578125, 371), (245.73590087890625, 101), (132.80133056640625, 249), (422.2581481933594, 249), (252.9711151123047, 218), (350.1542053222656, 263), (327.7568664550781, 349), (316.2778625488281, 469), (73.82817840576172, 106), (170.40174865722656, 213), (267.37347412109375, 305), (265.1798095703125, 256), (186.89918518066406, 273), (243.48573303222656, 75), (219.05828857421875, 264), (209.02621459960938, 189), (229.86526489257812, 313), (129.1331329345703, 32), (183.33203125, 242), (385.2653503417969, 370), (158.89852905273438, 217), (113.5411376953125, 28), (351.6352844238281, 241), (206.50244140625, 220), (150.26824951171875, 97), (176.29559326171875, 180), (362.85272216796875, 2)]\n",
      "pred: [(142.53115844726562, 55), (159.28858947753906, 128), (78.3479995727539, 26), (118.26432037353516, 169), (102.40924072265625, 153), (327.3758850097656, 272), (99.74979400634766, 379), (320.73480224609375, 106), (438.4451599121094, 110), (218.59754943847656, 317), (236.33200073242188, 167), (128.1773681640625, 145), (236.03480529785156, 90), (88.5436782836914, 2), (192.63531494140625, 58), (125.97310638427734, 142), (71.84488677978516, 46), (99.8354721069336, 59), (234.005615234375, 280), (292.65301513671875, 421), (234.9691619873047, 112), (366.7474670410156, 248), (273.4851379394531, 289), (239.26295471191406, 361), (320.45806884765625, 360), (354.8788146972656, 353), (106.8770980834961, 2), (70.65856170654297, 40), (266.51898193359375, 94), (207.0625457763672, 251), (351.33428955078125, 356), (83.25553131103516, 2)]\n",
      "pred: [(89.0379638671875, 32), (130.41778564453125, 27), (227.77992248535156, 249), (301.07989501953125, 289), (195.54359436035156, 430), (251.91090393066406, 306), (130.2557830810547, 125), (187.52674865722656, 339), (385.3829040527344, 729), (264.12322998046875, 108), (259.6540222167969, 319), (228.93016052246094, 162), (108.31694793701172, 20), (112.32174682617188, 85), (131.29954528808594, 52), (147.12498474121094, 133), (83.87108612060547, 11), (224.16644287109375, 276), (199.20420837402344, 138), (167.79359436035156, 77), (204.92166137695312, 416), (216.54800415039062, 296), (417.2297668457031, 281), (163.82740783691406, 83), (373.36492919921875, 631), (345.7572937011719, 259), (191.60951232910156, 172), (126.87593841552734, 124), (132.09523010253906, 205), (81.85884094238281, 76), (157.85125732421875, 20), (214.89744567871094, 132)]\n",
      "pred: [(246.69668579101562, 18), (139.68997192382812, 108), (287.86517333984375, 394), (262.9102783203125, 74), (278.3716735839844, 126), (249.23313903808594, 247), (270.59423828125, 292), (249.51329040527344, 348), (75.67254638671875, 21), (348.7707214355469, 243), (219.54901123046875, 233), (165.0434112548828, 99), (188.47775268554688, 268), (117.83262634277344, 139), (169.9231414794922, 196), (208.51077270507812, 368), (249.06935119628906, 286), (299.3016052246094, 254), (351.5039367675781, 219), (185.31263732910156, 88), (221.6239776611328, 189), (157.50991821289062, 142), (255.7803955078125, 314), (435.8221435546875, 588), (88.60694885253906, 158), (104.6733627319336, 19), (87.73661041259766, 11), (87.54401397705078, 99), (305.2591247558594, 347), (77.35456848144531, 186), (294.8289794921875, 344), (347.75115966796875, 455)]\n",
      "pred: [(330.2939758300781, 257), (266.7383117675781, 235), (392.3273620605469, 89), (175.66552734375, 266), (262.0603942871094, 278), (255.69813537597656, 91), (294.8918151855469, 285), (109.02523803710938, 133), (312.8283996582031, 182), (142.06491088867188, 116), (121.34234619140625, 102), (342.9796447753906, 583), (253.5214080810547, 315), (105.86813354492188, 86), (257.9844055175781, 158), (131.3797149658203, 85), (308.2359924316406, 361), (400.6114807128906, 569), (136.88792419433594, 123), (251.50338745117188, 226), (226.0875701904297, 183), (298.55108642578125, 212), (422.5021057128906, 563), (306.0187072753906, 249), (425.3821105957031, 313), (269.361572265625, 392), (120.67740631103516, 89), (86.51283264160156, 16), (292.5099792480469, 237), (279.2744445800781, 404), (296.79718017578125, 506), (302.0670166015625, 55)]\n",
      "pred: [(141.6461944580078, 111), (158.5694580078125, 175), (150.81552124023438, 187), (431.86468505859375, 279), (148.5237579345703, 74), (86.71590423583984, 14), (91.20238494873047, 100), (180.42950439453125, 187), (285.8970642089844, 158), (153.6170196533203, 77), (180.4016876220703, 171), (164.29225158691406, 303), (429.529052734375, 928), (310.37127685546875, 434), (182.841552734375, 197), (116.61022186279297, 11), (238.4620361328125, 398), (135.9047088623047, 189), (380.7496643066406, 360), (122.2234115600586, 72), (350.982421875, 199), (216.50131225585938, 206), (64.11537170410156, 48), (232.0836181640625, 382), (182.21041870117188, 90), (234.67335510253906, 244), (244.56358337402344, 112), (110.5860824584961, 98), (96.90196228027344, 356), (157.34011840820312, 496), (219.91094970703125, 75), (125.31792449951172, 76)]\n",
      "pred: [(108.64881896972656, 259), (186.3825225830078, 211), (237.61546325683594, 283), (211.20745849609375, 211), (97.89804077148438, 20), (94.95160675048828, 94), (170.2023162841797, 94), (181.16636657714844, 281), (252.30160522460938, 339), (262.4918212890625, 375), (113.67857360839844, 61), (261.7550048828125, 374), (169.50022888183594, 230), (87.64434051513672, 15), (182.1965789794922, 74), (270.6289978027344, 346), (274.52960205078125, 367), (421.483154296875, 618), (227.6373291015625, 577), (127.38432312011719, 59), (299.6148681640625, 438), (181.0860137939453, 176), (210.2901611328125, 83), (131.09671020507812, 228), (192.05514526367188, 66), (136.6824493408203, 110), (416.9256896972656, 609), (186.01133728027344, 384), (433.8702392578125, 319), (394.2181396484375, 176), (144.7066192626953, 88), (82.51876068115234, 112)]\n",
      "pred: [(245.5347900390625, 223), (85.12181854248047, 17), (86.4543228149414, 22), (362.51373291015625, 337), (249.4657745361328, 236), (137.1478271484375, 77), (100.60481262207031, 100), (270.2341003417969, 426), (341.00897216796875, 227), (320.4894714355469, 276), (239.82972717285156, 59), (259.2130432128906, 270), (208.46556091308594, 134), (257.1126708984375, 226), (92.7125015258789, 20), (364.4906005859375, 354), (78.61282348632812, 43), (79.47825622558594, 2), (99.7011489868164, 23), (202.76490783691406, 173), (186.265625, 296), (165.2664031982422, 108), (299.4192199707031, 273), (222.32565307617188, 331), (106.49469757080078, 34), (83.76998901367188, 40), (172.3591766357422, 163), (112.20677185058594, 48), (188.93971252441406, 117), (125.40939331054688, 220), (277.7174072265625, 205), (164.86627197265625, 281)]\n",
      "pred: [(213.41885375976562, 168), (282.25897216796875, 194), (78.23967742919922, 65), (129.4618682861328, 206), (169.25909423828125, 212), (304.7451477050781, 53), (257.0309143066406, 262), (383.01983642578125, 443), (284.645263671875, 267), (146.02963256835938, 182), (279.2671813964844, 200), (266.1089782714844, 221), (139.75340270996094, 61), (204.96246337890625, 62), (333.207275390625, 375), (122.37037658691406, 88), (202.37576293945312, 250), (119.79920959472656, 16), (264.6335754394531, 394), (247.0509796142578, 206), (231.0959930419922, 348), (135.8009033203125, 144), (314.45263671875, 150), (101.48208618164062, 86), (174.99716186523438, 142), (338.5146484375, 365), (119.65300750732422, 22), (59.075809478759766, 6), (152.73353576660156, 135), (108.98636627197266, 86), (105.81648254394531, 74), (186.84701538085938, 87)]\n",
      "pred: [(355.4541015625, 291), (109.61328887939453, 128), (73.0202865600586, 24), (297.5072326660156, 145), (179.740966796875, 19), (211.7259521484375, 193), (271.4839782714844, 182), (240.99844360351562, 23), (101.62611389160156, 43), (103.94831848144531, 90), (80.14698028564453, 100), (243.39466857910156, 117), (285.1493225097656, 104), (339.98004150390625, 308), (207.00421142578125, 290), (204.99920654296875, 260), (117.4836654663086, 77), (93.23831176757812, 474), (108.68331909179688, 96), (165.47103881835938, 36), (97.37410736083984, 13), (240.38819885253906, 139), (204.61048889160156, 60), (81.9063720703125, 62), (160.9590606689453, 41), (285.7202453613281, 234), (197.45387268066406, 233), (129.5128173828125, 2), (199.1087646484375, 82), (196.76904296875, 260), (346.76806640625, 233), (250.23709106445312, 329)]\n",
      "mean cross entropy loss:  tensor(12495.1426)\n"
     ]
    }
   ],
   "source": [
    "see_prediction(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "719c8a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "def evaluate_accuracy(model, dataloader, device='cpu'):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            logits, loss = model(input_ids, attention_mask, labels)\n",
    "            pred = logits\n",
    "\n",
    "            all_preds.extend(pred.cpu().tolist())\n",
    "            all_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "    mse = mean_squared_error(all_labels, all_preds)\n",
    "    return mse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c7b9c5fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 11504.4834\n"
     ]
    }
   ],
   "source": [
    "mse = evaluate_accuracy(model, val_dl, device=device)\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
