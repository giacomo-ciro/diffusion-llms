{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c8d6cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lgand\\anaconda3\\envs\\DiffuGPT\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from datamodule import PromptDataset, get_length_reg\n",
    "from model_baseline import DistilBertRegressor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84e63b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# === Tokenizer ===\n",
    "tokenizer_BERT = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "tokenizer_LLaDa = AutoTokenizer.from_pretrained('GSAI-ML/LLaDA-8B-Instruct', trust_remote_code=True)\n",
    "\n",
    "# I would have done with also 2048 and 4096 but in the training data there are no examples with that length\n",
    "\n",
    "# === Load data ===\n",
    "df_train = pd.read_csv(r\"..\\data\\train.csv\")\n",
    "train_data = list(zip(df_train[\"user_prompt\"], get_length_reg(df_train[\"model_response\"], tokenizer_LLaDa)))\n",
    "del df_train\n",
    "\n",
    "df_test = pd.read_csv(r\"..\\data\\test.csv\")\n",
    "df_test = df_test.dropna(subset=[\"model_response\"])\n",
    "data_test = list(zip(df_test[\"user_prompt\"], get_length_reg(df_test[\"model_response\"], tokenizer_LLaDa)))\n",
    "del df_test\n",
    "\n",
    "val_data, test_data = train_test_split(data_test, test_size=0.3, random_state=42)\n",
    "\n",
    "# All the training prompt except one have length < 64\n",
    "train_ds = PromptDataset(train_data, tokenizer_BERT, max_len=64)\n",
    "val_ds = PromptDataset(val_data, tokenizer_BERT, max_len=128)\n",
    "test_ds = PromptDataset(test_data, tokenizer_BERT, max_len=128)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=16)\n",
    "test_dl = DataLoader(test_ds, batch_size=16)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b92b673",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DistilBertRegressor()\n",
    "# model = torch.compile(model).to(device)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bce4cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(eval_iters = 10):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        if split == 'train':\n",
    "            dataloader = train_dl\n",
    "        else:\n",
    "            dataloader = val_dl\n",
    "        k = 0\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            logits, loss = model(input_ids, attention_mask, labels)\n",
    "            assert loss is not None, \"Loss should not be None\"\n",
    "            losses[k] = loss.item()\n",
    "            k += 1\n",
    "            if k >= eval_iters:\n",
    "                break\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea88e51",
   "metadata": {},
   "source": [
    "### Freezing the DistilBERT parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25b73e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.regressor.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57b26c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 53821.6680, val loss 56883.5703\n",
      "step 200: train loss 54077.6875, val loss 56035.7305\n",
      "step 400: train loss 51715.7578, val loss 54982.7812\n",
      "step 600: train loss 51472.6094, val loss 53731.6680\n",
      "step 800: train loss 50682.9062, val loss 52255.3711\n",
      "step 890: train loss 54525.1992, val loss 51507.0586\n",
      "[Epoch 1] Loss: 47106147.9385\n",
      "step 0: train loss 57997.3828, val loss 51499.2109\n",
      "step 200: train loss 50459.4648, val loss 49719.6914\n",
      "step 400: train loss 35980.2891, val loss 47815.9922\n",
      "step 600: train loss 50268.2773, val loss 45851.4609\n",
      "step 800: train loss 43603.1641, val loss 43753.4688\n",
      "step 890: train loss 36718.3203, val loss 42808.6953\n",
      "[Epoch 2] Loss: 40830067.4756\n",
      "step 0: train loss 44452.1250, val loss 42797.9805\n",
      "step 200: train loss 35735.5391, val loss 40642.3359\n",
      "step 400: train loss 30312.0117, val loss 38554.7578\n",
      "step 600: train loss 35604.3047, val loss 36463.1055\n",
      "step 800: train loss 27563.8008, val loss 34393.5781\n",
      "step 890: train loss 28638.4414, val loss 33509.2812\n",
      "[Epoch 3] Loss: 32814198.9302\n",
      "step 0: train loss 33498.2930, val loss 33499.8828\n",
      "step 200: train loss 30741.4004, val loss 31544.6152\n",
      "step 400: train loss 28929.3691, val loss 29674.7285\n",
      "step 600: train loss 24357.1289, val loss 27844.1152\n",
      "step 800: train loss 18132.7363, val loss 26184.4648\n",
      "step 890: train loss 21419.0059, val loss 25493.3555\n",
      "[Epoch 4] Loss: 25091797.3999\n",
      "step 0: train loss 23118.1250, val loss 25486.4883\n",
      "step 200: train loss 18624.5820, val loss 24027.9980\n",
      "step 400: train loss 18787.8945, val loss 22666.2852\n",
      "step 600: train loss 18509.1445, val loss 21426.9570\n",
      "step 800: train loss 17583.1465, val loss 20340.9746\n",
      "step 890: train loss 16793.6738, val loss 19899.0684\n",
      "[Epoch 5] Loss: 18986542.9688\n",
      "step 0: train loss 17257.5781, val loss 19893.8789\n",
      "step 200: train loss 20909.9238, val loss 19033.6035\n",
      "step 400: train loss 19339.1992, val loss 18312.3242\n",
      "step 600: train loss 15770.5654, val loss 17674.1270\n",
      "step 800: train loss 14779.6621, val loss 17133.9297\n",
      "step 890: train loss 18344.9277, val loss 16940.4160\n",
      "[Epoch 6] Loss: 15153114.4648\n",
      "step 0: train loss 15693.1826, val loss 16938.7148\n",
      "step 200: train loss 13667.6221, val loss 16579.4531\n",
      "step 400: train loss 15083.2373, val loss 16301.0605\n",
      "step 600: train loss 13822.4014, val loss 16113.9277\n",
      "step 800: train loss 15980.4824, val loss 15961.1094\n",
      "step 890: train loss 12190.0537, val loss 15923.5566\n",
      "[Epoch 7] Loss: 13315515.8730\n",
      "step 0: train loss 13399.9473, val loss 15923.0566\n",
      "step 200: train loss 13455.7217, val loss 15820.9639\n",
      "step 400: train loss 13766.5215, val loss 15769.6377\n",
      "step 600: train loss 12531.9238, val loss 15727.4199\n",
      "step 800: train loss 12587.1357, val loss 15692.5986\n",
      "step 890: train loss 14417.9639, val loss 15680.2344\n",
      "[Epoch 8] Loss: 12675817.7568\n",
      "step 0: train loss 11708.0645, val loss 15680.0908\n",
      "step 200: train loss 11619.0391, val loss 15658.8643\n",
      "step 400: train loss 14453.0137, val loss 15635.2988\n",
      "step 600: train loss 12217.9678, val loss 15610.9316\n",
      "step 800: train loss 13918.2676, val loss 15592.3799\n",
      "step 890: train loss 14591.5332, val loss 15581.3311\n",
      "[Epoch 9] Loss: 12493839.1235\n",
      "step 0: train loss 13066.5596, val loss 15581.2480\n",
      "step 200: train loss 13871.2607, val loss 15568.8564\n",
      "step 400: train loss 12680.4668, val loss 15540.0332\n",
      "step 600: train loss 15882.2754, val loss 15513.9707\n",
      "step 800: train loss 15042.3330, val loss 15489.9688\n",
      "step 890: train loss 12043.7822, val loss 15476.7324\n",
      "[Epoch 10] Loss: 12378701.9180\n"
     ]
    }
   ],
   "source": [
    "eval_interval = 200\n",
    "max_iters = len(train_dl)\n",
    "\n",
    "# === Training loop ===\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for i, batch in enumerate(train_dl):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        logits, loss = model(input_ids, attention_mask, labels)\n",
    "        assert loss is not None, \"Loss should not be None\"\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        if i % eval_interval == 0 or i == max_iters - 1:\n",
    "                losses = estimate_loss()\n",
    "                print(f\"step {i}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    print(f\"[Epoch {epoch+1}] Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cc092c",
   "metadata": {},
   "source": [
    "### Fine tuning: changing also the parameters of DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4386c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Congela tutti i parametri di BERT\n",
    "for param in model.encoder.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c2700d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 11786.5625, val loss 15483.3203\n",
      "step 200: train loss 12569.9678, val loss 13120.4922\n",
      "step 400: train loss 8993.9922, val loss 11818.5000\n",
      "step 600: train loss 11440.7998, val loss 11661.8643\n",
      "step 800: train loss 10326.2275, val loss 11394.1084\n",
      "step 890: train loss 7899.3687, val loss 11435.4277\n",
      "[Epoch 1] Loss: 9954936.4500\n",
      "step 0: train loss 6805.6108, val loss 11389.7490\n",
      "step 200: train loss 8292.1143, val loss 11462.5674\n",
      "step 400: train loss 10395.0547, val loss 10864.5762\n",
      "step 600: train loss 8008.9819, val loss 10445.2129\n",
      "step 800: train loss 10152.5605, val loss 10665.1387\n",
      "step 890: train loss 9229.6250, val loss 10639.5869\n",
      "[Epoch 2] Loss: 8611750.4399\n",
      "step 0: train loss 9773.0293, val loss 10644.4980\n",
      "step 200: train loss 9010.0645, val loss 10362.3408\n",
      "step 400: train loss 9648.1982, val loss 10408.4629\n",
      "step 600: train loss 9706.8457, val loss 10232.0713\n",
      "step 800: train loss 6735.9639, val loss 10523.7080\n",
      "step 890: train loss 7574.8921, val loss 10656.4375\n",
      "[Epoch 3] Loss: 7974715.9598\n",
      "step 0: train loss 9059.1934, val loss 10671.0928\n",
      "step 200: train loss 8229.9219, val loss 10363.9404\n",
      "step 400: train loss 8396.9014, val loss 10092.5186\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     16\u001b[39m loss.backward()\n\u001b[32m     17\u001b[39m optimizer.step()\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m total_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m i % eval_interval == \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m i == max_iters - \u001b[32m1\u001b[39m:\n\u001b[32m     20\u001b[39m         losses = estimate_loss()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "eval_interval = 200\n",
    "max_iters = len(train_dl)\n",
    "\n",
    "# === Training loop ===\n",
    "for epoch in range(3):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for i, batch in enumerate(train_dl):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        logits, loss = model(input_ids, attention_mask, labels)\n",
    "        assert loss is not None, \"Loss should not be None\"\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        if i % eval_interval == 0 or i == max_iters - 1:\n",
    "                losses = estimate_loss()\n",
    "                print(f\"step {i}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    print(f\"[Epoch {epoch+1}] Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed108574",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4bf674d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 7409.3125, val loss 10114.3750\n",
      "step 200: train loss 8863.0352, val loss 10173.2363\n",
      "step 400: train loss 7386.7437, val loss 10268.1855\n",
      "step 600: train loss 7325.1265, val loss 10288.1348\n",
      "step 800: train loss 6753.1538, val loss 10148.7363\n",
      "step 890: train loss 5931.0869, val loss 10062.7529\n",
      "[Epoch 1] Loss: 7347738.3793\n"
     ]
    }
   ],
   "source": [
    "eval_interval = 200\n",
    "max_iters = len(train_dl)\n",
    "\n",
    "# === Training loop ===\n",
    "for epoch in range(1):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for i, batch in enumerate(train_dl):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        logits, loss = model(input_ids, attention_mask, labels)\n",
    "        assert loss is not None, \"Loss should not be None\"\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        if i % eval_interval == 0 or i == max_iters - 1:\n",
    "                losses = estimate_loss()\n",
    "                print(f\"step {i}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    print(f\"[Epoch {epoch+1}] Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b707148",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"checkpoints/DistilBERT_LLaDa_reg.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb70fb9",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b97ed5b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DistilBertRegressor()\n",
    "model.load_state_dict(torch.load(\"checkpoints/DistilBERT_LLaDa_reg.pth\"))\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05c8187b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([217.7619], device='cuda:0', grad_fn=<SqueezeBackward1>), None)\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Can you explain the theory of relativity?\"\n",
    "input_enc = tokenizer_BERT(input_text, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=512).to(device)\n",
    "output = model(input_enc['input_ids'], input_enc['attention_mask'])\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0b2c15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([46.9342], device='cuda:0', grad_fn=<SqueezeBackward1>), None)\n"
     ]
    }
   ],
   "source": [
    "input_text = \"What's your name?\"\n",
    "input_enc = tokenizer_BERT(input_text, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=512).to(device)\n",
    "output = model(input_enc['input_ids'], input_enc['attention_mask'])\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1e215d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([108.0012], device='cuda:0', grad_fn=<SqueezeBackward1>), None)\n"
     ]
    }
   ],
   "source": [
    "input_text = \"What is 3 + 3?\"\n",
    "input_enc = tokenizer_BERT(input_text, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=512).to(device)\n",
    "output = model(input_enc['input_ids'], input_enc['attention_mask'])\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b44a252f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def see_prediction(kappa = 1):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['val']:\n",
    "        losses = torch.zeros(kappa)\n",
    "        if split == 'train':\n",
    "            dataloader = train_dl\n",
    "        else:\n",
    "            dataloader = val_dl\n",
    "        k = 0\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            logits, loss = model(input_ids, attention_mask, labels)\n",
    "            pred = logits\n",
    "            print(f\"pred: {[(pred[i].item(), labels[i].item()) for i in range(16)]}\")\n",
    "            assert loss is not None, \"Loss should not be None\"\n",
    "            losses[k] = loss.item()\n",
    "            k += 1\n",
    "            if k >= kappa:\n",
    "                break\n",
    "        out[split] = losses\n",
    "    model.train()\n",
    "    print(\"mean cross entropy loss: \", out[\"val\"].mean())\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e284c01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: [(67.27046203613281, 269), (126.904052734375, 47), (84.86556243896484, 75), (156.15689086914062, 158), (76.28671264648438, 53), (295.21954345703125, 348), (231.06982421875, 102), (130.68165588378906, 240), (329.534423828125, 234), (213.53468322753906, 216), (312.6249694824219, 260), (319.0155029296875, 345), (293.59295654296875, 454), (59.38877487182617, 106), (163.07162475585938, 210), (267.2560119628906, 303)]\n",
      "pred: [(247.94784545898438, 255), (198.24847412109375, 260), (247.11895751953125, 75), (209.23394775390625, 267), (181.27635192871094, 189), (200.6521453857422, 310), (129.39602661132812, 30), (179.17127990722656, 242), (364.22772216796875, 362), (156.38255310058594, 219), (101.56836700439453, 28), (280.0093994140625, 221), (194.967041015625, 215), (152.21661376953125, 101), (159.84963989257812, 175), (318.2129211425781, 2)]\n",
      "pred: [(122.97423553466797, 64), (163.20803833007812, 125), (73.89338684082031, 26), (117.00796508789062, 167), (92.34988403320312, 153), (271.68048095703125, 224), (90.11426544189453, 375), (295.1036682128906, 104), (384.6766052246094, 104), (190.82406616210938, 317), (220.97088623046875, 166), (132.93218994140625, 150), (220.10540771484375, 87), (78.77181243896484, 2), (177.7879638671875, 60), (113.30592346191406, 142)]\n",
      "pred: [(61.408748626708984, 46), (92.15213012695312, 59), (227.04345703125, 280), (285.7942810058594, 415), (219.9752197265625, 111), (314.76171875, 239), (253.65171813964844, 280), (208.92105102539062, 357), (284.5700988769531, 320), (299.0094909667969, 340), (101.03557586669922, 2), (57.461727142333984, 44), (236.8267822265625, 93), (184.95465087890625, 251), (332.1700439453125, 354), (67.92792510986328, 2)]\n",
      "pred: [(88.05369567871094, 32), (122.73478698730469, 27), (223.1065673828125, 243), (273.78173828125, 299), (201.9486083984375, 426), (241.0108184814453, 305), (121.33821105957031, 127), (179.85812377929688, 339), (356.0289001464844, 463), (260.97723388671875, 108), (247.4098663330078, 314), (204.5504150390625, 156), (94.83426666259766, 20), (115.49027252197266, 85), (127.16986846923828, 52), (121.35900115966797, 120)]\n",
      "pred: [(79.96379852294922, 16), (216.1236572265625, 270), (197.77749633789062, 135), (155.78952026367188, 77), (192.22756958007812, 317), (209.84854125976562, 297), (330.1568908691406, 274), (164.2498779296875, 84), (335.55657958984375, 507), (295.46697998046875, 251), (196.72479248046875, 171), (139.68923950195312, 131), (121.04534912109375, 202), (75.15502166748047, 75), (159.4032745361328, 19), (207.87294006347656, 132)]\n",
      "pred: [(237.13088989257812, 18), (135.99514770507812, 108), (275.105712890625, 392), (278.42584228515625, 73), (300.3245849609375, 124), (234.97328186035156, 244), (263.7107238769531, 291), (247.5753631591797, 345), (66.62400817871094, 25), (333.11138916015625, 235), (207.62551879882812, 240), (151.88583374023438, 98), (177.33453369140625, 282), (106.8480224609375, 139), (159.39913940429688, 199), (210.9359130859375, 410)]\n",
      "pred: [(206.24276733398438, 283), (280.665771484375, 233), (277.86700439453125, 187), (191.59976196289062, 91), (224.8666229248047, 190), (170.08749389648438, 143), (246.4579620361328, 314), (388.35357666015625, 514), (85.02415466308594, 175), (86.84474182128906, 19), (81.69230651855469, 9), (80.65325927734375, 122), (296.5443115234375, 318), (71.19892120361328, 185), (274.2782897949219, 343), (306.7265625, 401)]\n",
      "pred: [(307.314697265625, 261), (250.8280029296875, 231), (326.798095703125, 88), (162.87591552734375, 266), (289.2409362792969, 291), (228.25563049316406, 92), (291.2061767578125, 284), (107.6304931640625, 133), (317.616455078125, 184), (125.48151397705078, 116), (117.43675231933594, 102), (308.72564697265625, 465), (249.89212036132812, 309), (100.31260681152344, 86), (256.35662841796875, 158), (131.79714965820312, 85)]\n",
      "pred: [(299.869873046875, 360), (343.1290283203125, 511), (130.87918090820312, 120), (234.71185302734375, 224), (215.55523681640625, 182), (253.5712890625, 208), (378.60223388671875, 498), (287.2178955078125, 244), (367.7034912109375, 286), (250.48028564453125, 389), (122.4156494140625, 89), (85.59408569335938, 16), (278.6963195800781, 217), (278.611572265625, 472), (270.0935363769531, 468), (284.21075439453125, 55)]\n",
      "mean cross entropy loss:  tensor(10062.7529)\n"
     ]
    }
   ],
   "source": [
    "see_prediction(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "719c8a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "def evaluate_accuracy(model, dataloader, device='cpu'):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            logits, loss = model(input_ids, attention_mask, labels)\n",
    "            pred = logits\n",
    "\n",
    "            all_preds.extend(pred.cpu().tolist())\n",
    "            all_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "    mse = mean_squared_error(all_labels, all_preds)\n",
    "    return mse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c7b9c5fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 9267.0477\n"
     ]
    }
   ],
   "source": [
    "mse = evaluate_accuracy(model, val_dl, device=device)\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c966c2b3",
   "metadata": {},
   "source": [
    "### Final evaluation: obtaining the list of predictions on the test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c2ee6b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 3)\n",
      "(4998, 3)\n"
     ]
    }
   ],
   "source": [
    "df_test = pd.read_csv(r\"..\\data\\test.csv\")\n",
    "print(df_test.shape)\n",
    "df_test = df_test.dropna(subset=[\"model_response\"])\n",
    "print(df_test.shape)\n",
    "data_test = list(zip(df_test[\"user_prompt\"], get_length_reg(df_test[\"model_response\"], tokenizer_LLaDa)))\n",
    "del df_test\n",
    "\n",
    "\n",
    "test_ds = PromptDataset(data_test, tokenizer_BERT, max_len=64)\n",
    "test_dl = DataLoader(test_ds, batch_size=16)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "573ecc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, dataloader, device='cpu'):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            pred, loss = model(input_ids, attention_mask, labels)\n",
    "\n",
    "            all_preds.extend(pred.cpu().tolist())\n",
    "            all_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "    return all_preds, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b0597711",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds, all_labels = get_predictions(model, test_dl, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7be81443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([150., 300.,  98., ..., 133., 316., 155.]),\n",
       " array([ 28,  65, 154, ..., 111, 359,  28]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.ceil(np.array(all_preds)), np.array(all_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b40b4b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 9013.2199\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "all_preds = np.ceil(np.array(all_preds))\n",
    "all_labels = np.array(all_labels)\n",
    "mse = mean_squared_error(all_labels, all_preds)\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5418f0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(\"prediction_test\", exist_ok=True)\n",
    "\n",
    "# Save all_preds as a numpy array\n",
    "np.save(\"prediction_test/DistilBERT_LLaDa_reg.npy\", all_preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DiffuGPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
