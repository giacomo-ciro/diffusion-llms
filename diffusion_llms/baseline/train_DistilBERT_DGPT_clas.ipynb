{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c8d6cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from datamodule import PromptDataset, get_length\n",
    "from model_baseline import DistilBertClassifier\n",
    "import tiktoken\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84e63b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lgand\\anaconda3\\envs\\DL\\Lib\\site-packages\\huggingface_hub\\file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# === Tokenizer ===\n",
    "tokenizer_BERT = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "tokenizer_GPT2 = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "steps = [32, 64, 128, 256, 512]\n",
    "\n",
    "# === Load data ===\n",
    "df_train = pd.read_csv(r\"..\\data\\train.csv\")\n",
    "train_data = list(zip(df_train[\"user_prompt\"], get_length(df_train[\"model_response\"], tokenizer_GPT2, max_length=512, steps= steps)))\n",
    "del df_train\n",
    "\n",
    "df_test = pd.read_csv(r\"..\\data\\test.csv\")\n",
    "df_test = df_test.dropna(subset=[\"model_response\"])\n",
    "data_test = list(zip(df_test[\"user_prompt\"], get_length(df_test[\"model_response\"], tokenizer_GPT2, max_length= 512, steps= steps)))\n",
    "del df_test\n",
    "\n",
    "val_data, test_data = train_test_split(data_test, test_size=0.3, random_state=42)\n",
    "\n",
    "# All the training prompt except one have length < 64\n",
    "train_ds = PromptDataset(train_data, tokenizer_BERT, max_len=64)\n",
    "val_ds = PromptDataset(val_data, tokenizer_BERT, max_len=128)\n",
    "test_ds = PromptDataset(test_data, tokenizer_BERT, max_len=128)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=32)\n",
    "test_dl = DataLoader(test_ds, batch_size=32)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b0271f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DistilBertClassifier(n_classes=5)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bce4cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(eval_iters = 10):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        if split == 'train':\n",
    "            dataloader = train_dl\n",
    "        else:\n",
    "            dataloader = val_dl\n",
    "        k = 0\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            logits, loss = model(input_ids, attention_mask, labels)\n",
    "            assert loss is not None, \"Loss should not be None\"\n",
    "            losses[k] = loss.item()\n",
    "            k += 1\n",
    "            if k >= eval_iters:\n",
    "                break\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea88e51",
   "metadata": {},
   "source": [
    "### Freezing the DistilBERT parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25b73e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.classifier.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b26c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_interval = 200\n",
    "max_iters = len(train_dl)\n",
    "\n",
    "# === Training loop ===\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for i, batch in enumerate(train_dl):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        logits, loss = model(input_ids, attention_mask, labels)\n",
    "        assert loss is not None, \"Loss should not be None\"\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        if i % eval_interval == 0 or i == max_iters - 1:\n",
    "                losses = estimate_loss()\n",
    "                print(f\"step {i}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    print(f\"[Epoch {epoch+1}] Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "edc603c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.classifier.parameters(), lr=1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3bcd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_interval = 200\n",
    "max_iters = len(train_dl)\n",
    "\n",
    "# === Training loop ===\n",
    "for epoch in range(15):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for i, batch in enumerate(train_dl):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        logits, loss = model(input_ids, attention_mask, labels)\n",
    "        assert loss is not None, \"Loss should not be None\"\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        if i % eval_interval == 0 or i == max_iters - 1:\n",
    "                losses = estimate_loss()\n",
    "                print(f\"step {i}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    print(f\"[Epoch {epoch+1}] Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cc092c",
   "metadata": {},
   "source": [
    "### Fine tuning: changing also the parameters of DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4386c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Congela tutti i parametri di BERT\n",
    "for param in model.encoder.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c2700d",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_interval = 200\n",
    "max_iters = len(train_dl)\n",
    "\n",
    "# === Training loop ===\n",
    "for epoch in range(8):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for i, batch in enumerate(train_dl):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        logits, loss = model(input_ids, attention_mask, labels)\n",
    "        assert loss is not None, \"Loss should not be None\"\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        if i % eval_interval == 0 or i == max_iters - 1:\n",
    "                losses = estimate_loss()\n",
    "                print(f\"step {i}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    print(f\"[Epoch {epoch+1}] Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b707148",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"checkpoints/DistilBERT_1.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb70fb9",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97ed5b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DistilBertClassifier(n_classes=5)\n",
    "model.load_state_dict(torch.load(\"checkpoints/DistilBERT_1.pth\"))\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c8187b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[-3.0026, -1.5511,  0.2183,  1.3869,  1.6006]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>), None)\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Can you explain the theory of relativity?\"\n",
    "input_enc = tokenizer_BERT(input_text, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=512).to(device)\n",
    "output = model(input_enc['input_ids'], input_enc['attention_mask'])\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b2c15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[ 3.0997,  2.6984,  1.7371, -2.1407, -5.9127]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>), None)\n"
     ]
    }
   ],
   "source": [
    "input_text = \"What's your name?\"\n",
    "input_enc = tokenizer_BERT(input_text, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=512).to(device)\n",
    "output = model(input_enc['input_ids'], input_enc['attention_mask'])\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e215d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[ 1.2852,  0.6386,  1.1362, -0.5773, -2.8144]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>), None)\n"
     ]
    }
   ],
   "source": [
    "input_text = \"What is 3 + 3?\"\n",
    "input_enc = tokenizer_BERT(input_text, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=512).to(device)\n",
    "output = model(input_enc['input_ids'], input_enc['attention_mask'])\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b44a252f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def see_prediction(kappa = 1):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['val']:\n",
    "        losses = torch.zeros(kappa)\n",
    "        if split == 'train':\n",
    "            dataloader = train_dl\n",
    "        else:\n",
    "            dataloader = val_dl\n",
    "        k = 0\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            logits, loss = model(input_ids, attention_mask, labels)\n",
    "            pred = torch.argmax(logits, dim=1)\n",
    "            print(f\"pred: {[(pred[i].item(), labels[i].item()) for i in range(32)]}\")\n",
    "            assert loss is not None, \"Loss should not be None\"\n",
    "            losses[k] = loss.item()\n",
    "            k += 1\n",
    "            if k >= kappa:\n",
    "                break\n",
    "        out[split] = losses\n",
    "    model.train()\n",
    "    print(\"mean cross entropy loss: \", out[\"val\"].mean())\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e284c01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: [(2, 1), (2, 2), (4, 3), (2, 3), (1, 0), (3, 3), (3, 3), (3, 4), (4, 4), (2, 2), (2, 2), (3, 3), (4, 4), (4, 4), (0, 0), (2, 4), (3, 4), (3, 3), (2, 2), (4, 3), (3, 2), (4, 2), (4, 4), (2, 3), (3, 4), (1, 2), (4, 3), (4, 4), (4, 4), (3, 3), (3, 3), (2, 2)]\n",
      "pred: [(2, 2), (4, 3), (3, 2), (2, 0), (4, 4), (4, 4), (3, 3), (3, 2), (2, 4), (4, 4), (4, 3), (2, 2), (4, 3), (3, 0), (4, 4), (3, 3), (3, 3), (3, 4), (4, 3), (2, 2), (3, 3), (3, 2), (3, 3), (1, 1), (4, 4), (3, 2), (1, 0), (3, 4), (4, 4), (2, 1), (3, 3), (3, 0)]\n",
      "pred: [(3, 4), (3, 3), (4, 4), (4, 2), (3, 2), (2, 2), (2, 2), (3, 3), (0, 0), (1, 1), (4, 4), (4, 4), (4, 3), (4, 4), (3, 1), (3, 3), (4, 3), (3, 2), (4, 4), (3, 3), (3, 2), (4, 4), (3, 4), (3, 3), (2, 3), (3, 3), (4, 2), (4, 3), (3, 2), (2, 2), (3, 4), (4, 4)]\n",
      "pred: [(3, 4), (2, 0), (4, 4), (2, 0), (2, 2), (3, 4), (4, 3), (4, 4), (4, 4), (4, 3), (2, 3), (0, 0), (3, 2), (3, 4), (2, 2), (0, 1), (3, 3), (2, 2), (3, 2), (2, 2), (0, 0), (3, 3), (2, 2), (4, 3), (4, 4), (3, 3), (3, 4), (4, 4), (2, 3), (4, 3), (4, 3), (4, 4)]\n",
      "pred: [(3, 3), (3, 2), (3, 2), (3, 4), (4, 3), (2, 0), (3, 4), (4, 2), (3, 2), (3, 3), (3, 3), (3, 3), (2, 2), (4, 4), (3, 4), (3, 4), (2, 1), (2, 3), (4, 4), (4, 3), (3, 4), (3, 3), (4, 4), (3, 4), (3, 4), (2, 3), (2, 2), (2, 2), (3, 3), (4, 4), (3, 3), (3, 2)]\n",
      "pred: [(2, 2), (3, 4), (2, 2), (2, 3), (0, 1), (3, 3), (3, 4), (4, 4), (2, 2), (4, 4), (3, 3), (3, 2), (1, 1), (3, 4), (2, 2), (3, 1), (4, 4), (2, 2), (3, 4), (4, 4), (4, 4), (3, 3), (3, 0), (2, 3), (3, 1), (4, 4), (3, 2), (3, 3), (3, 3), (2, 3), (3, 4), (3, 4)]\n",
      "pred: [(3, 3), (3, 3), (3, 1), (4, 3), (3, 1), (3, 4), (4, 4), (2, 4), (0, 1), (3, 2), (3, 3), (3, 3), (1, 1), (3, 3), (4, 4), (3, 4), (3, 4), (0, 2), (1, 4), (3, 3), (4, 4), (2, 2), (0, 3), (3, 3), (2, 2), (3, 4), (3, 3), (4, 3), (2, 3), (3, 3), (0, 0), (0, 2)]\n",
      "pred: [(3, 4), (4, 3), (0, 3), (4, 3), (3, 2), (3, 4), (3, 3), (4, 2), (4, 4), (3, 3), (4, 3), (3, 3), (4, 4), (3, 4), (3, 3), (4, 3), (4, 3), (0, 0), (3, 0), (3, 4), (2, 1), (4, 4), (0, 0), (4, 3), (4, 3), (3, 3), (4, 4), (2, 3), (4, 4), (4, 4), (4, 2), (4, 4)]\n",
      "pred: [(3, 3), (3, 2), (4, 4), (4, 4), (4, 4), (3, 3), (3, 2), (3, 2), (4, 3), (2, 2), (4, 4), (0, 1), (3, 3), (4, 4), (2, 2), (3, 4), (2, 2), (3, 4), (3, 2), (4, 4), (3, 3), (4, 4), (3, 2), (4, 3), (4, 4), (4, 3), (0, 3), (4, 4), (3, 2), (4, 4), (2, 2), (2, 1)]\n",
      "pred: [(2, 3), (3, 3), (2, 3), (2, 3), (4, 2), (3, 3), (3, 3), (3, 3), (3, 3), (3, 4), (4, 3), (3, 4), (4, 4), (3, 1), (3, 2), (4, 4), (0, 0), (4, 4), (3, 3), (3, 4), (3, 1), (3, 3), (2, 2), (0, 0), (4, 4), (4, 4), (2, 2), (3, 2), (3, 3), (3, 3), (3, 2), (2, 2)]\n",
      "mean cross entropy loss:  tensor(1.1203)\n"
     ]
    }
   ],
   "source": [
    "see_prediction(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "719c8a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "def evaluate_accuracy(model, dataloader, device='cpu'):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            logits, loss = model(input_ids, attention_mask, labels)\n",
    "            pred = torch.argmax(logits, dim=1)\n",
    "\n",
    "            all_preds.extend(pred.cpu().tolist())\n",
    "            all_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    mse = mean_squared_error(all_labels, all_preds)\n",
    "    return accuracy, mse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c7b9c5fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 48.67%\n",
      "Mean Squared Error: 1.0368\n"
     ]
    }
   ],
   "source": [
    "acc, mse = evaluate_accuracy(model, val_dl, device=device)\n",
    "print(f\"Accuracy: {acc:.2%}\")\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
