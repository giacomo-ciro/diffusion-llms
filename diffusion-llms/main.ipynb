{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing new GPT-2...\n"
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import model\n",
    "import torch\n",
    "reload(model)\n",
    "\n",
    "model = model.GPT2(\"./config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 10, 50257]) torch.Size([3, 10])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "GPT2 is not attached to a `Trainer`.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m input_ids = sentence[:, :-\u001b[32m1\u001b[39m]\n\u001b[32m     10\u001b[39m targets = sentence[:, \u001b[32m1\u001b[39m:]\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m logits = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/diffusion-llms/diffusion-llms/model.py:108\u001b[39m, in \u001b[36mGPT2.training_step\u001b[39m\u001b[34m(self, batch, batch_idx)\u001b[39m\n\u001b[32m    106\u001b[39m \u001b[38;5;66;03m# Logging\u001b[39;00m\n\u001b[32m    107\u001b[39m \u001b[38;5;28mself\u001b[39m.log(\u001b[33m\"\u001b[39m\u001b[33mtrain/loss\u001b[39m\u001b[33m\"\u001b[39m, loss, on_step=\u001b[38;5;28;01mTrue\u001b[39;00m, on_epoch=\u001b[38;5;28;01mFalse\u001b[39;00m, prog_bar=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m current_lr = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptimizers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.param_groups[\u001b[32m0\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mlr\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m    109\u001b[39m \u001b[38;5;28mself\u001b[39m.log(\u001b[33m'\u001b[39m\u001b[33mlearning_rate\u001b[39m\u001b[33m'\u001b[39m, current_lr, prog_bar=\u001b[38;5;28;01mTrue\u001b[39;00m, on_step=\u001b[38;5;28;01mTrue\u001b[39;00m, on_epoch=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/diffu/lib/python3.12/site-packages/lightning/pytorch/core/module.py:174\u001b[39m, in \u001b[36mLightningModule.optimizers\u001b[39m\u001b[34m(self, use_pl_optimizer)\u001b[39m\n\u001b[32m    172\u001b[39m     opts: MODULE_OPTIMIZERS = \u001b[38;5;28mself\u001b[39m._fabric_optimizers\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m use_pl_optimizer:\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m     opts = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrainer\u001b[49m.strategy._lightning_optimizers\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    176\u001b[39m     opts = \u001b[38;5;28mself\u001b[39m.trainer.optimizers\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/diffu/lib/python3.12/site-packages/lightning/pytorch/core/module.py:214\u001b[39m, in \u001b[36mLightningModule.trainer\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    212\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _TrainerFabricShim(fabric=\u001b[38;5;28mself\u001b[39m._fabric)  \u001b[38;5;66;03m# type: ignore[return-value]\u001b[39;00m\n\u001b[32m    213\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_is_scripting \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._trainer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is not attached to a `Trainer`.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._trainer\n",
      "\u001b[31mRuntimeError\u001b[39m: GPT2 is not attached to a `Trainer`."
     ]
    }
   ],
   "source": [
    "p = 0.8\n",
    "context_length = 10\n",
    "B = 3\n",
    "attn_mask = torch.tril(torch.ones(context_length, context_length)).to(torch.bool)\n",
    "attn_unmask = torch.rand(size=(context_length, context_length)) <= p\n",
    "ans = attn_mask | attn_unmask\n",
    "\n",
    "sentence = torch.randint(0, 10, size = (B, context_length + 1))\n",
    "input_ids = sentence[:, :-1]\n",
    "targets = sentence[:, 1:]\n",
    "logits = model.training_step((input_ids, targets), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def get_anneal_attn_mask(seq_len, bsz, dtype, device, attn_mask_ratio):\n",
    "    mask = torch.full((seq_len, seq_len), 0, device=device)\n",
    "    mask_cond = torch.arange(mask.size(-1), device=device)\n",
    "    mask.masked_fill_(mask_cond < (mask_cond + 1).view(mask.size(-1), 1), 1)\n",
    "    causal_mask = mask.to(dtype)\n",
    "    \n",
    "    random_mask = torch.bernoulli(torch.full((seq_len, seq_len), 0.0, device=device) + attn_mask_ratio)\n",
    "\n",
    "    anneal_mask = torch.logical_or(causal_mask, random_mask)\n",
    "    expanded_mask = anneal_mask[None, None, :, :].expand(bsz, 1, seq_len, seq_len)\n",
    "    inverted_mask = 1.0 - expanded_mask.to(dtype)\n",
    "\n",
    "    return inverted_mask.masked_fill(inverted_mask.to(torch.bool), torch.finfo(dtype).min)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
