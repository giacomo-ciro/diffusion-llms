LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
Loading `train_dataloader` to estimate number of stepping batches.
/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=223` in the `DataLoader` to improve performance.

  | Name     | Type            | Params | Mode
-----------------------------------------------------
0 | backbone | LladaBackbone   | 8.0 B  | train
1 | model    | LLaDaClassifier | 8.4 M  | train
-----------------------------------------------------
526 M     Trainable params
7.5 B     Non-trainable params
8.0 B     Total params
32,095.896Total estimated model params size (MB)
6         Modules in train mode
422       Modules in eval mode
Epoch 0:  24%|███████████████                                                 | 200/847 [03:19<10:46,  1.00it/s, v_num=4u3d, train/loss=0.385, train/acc=0.568]
/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=223` in the `DataLoader` to improve performance.
Validation DataLoader 0: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 45/45 [00:44<00:00,  1.01it/s]
Traceback (most recent call last):
  File "/usr/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/usr/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/workspace/diffusion-llms/diffusion_llms/train_llada_pl.py", line 567, in <module>
    main()
  File "/workspace/diffusion-llms/diffusion_llms/train_llada_pl.py", line 551, in main
    trainer.fit(model,
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py", line 561, in fit
    call._call_and_handle_interrupt(
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py", line 48, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py", line 599, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py", line 1012, in _run
    results = self._run_stage()
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py", line 1056, in _run_stage
    self.fit_loop.run()
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py", line 216, in run
    self.advance()
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py", line 455, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/training_epoch_loop.py", line 151, in run
    self.on_advance_end(data_fetcher)
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/training_epoch_loop.py", line 370, in on_advance_end
    self.val_loop.run()
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py", line 179, in _decorator
    return loop_run(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/evaluation_loop.py", line 152, in run
    return self.on_run_end()
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/evaluation_loop.py", line 309, in on_run_end
    self._on_evaluation_end()
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/evaluation_loop.py", line 354, in _on_evaluation_end
    call._call_callback_hooks(trainer, hook_name, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py", line 227, in _call_callback_hooks
    fn(trainer, trainer.lightning_module, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/callbacks/early_stopping.py", line 196, in on_validation_end
    self._run_early_stopping_check(trainer)
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/callbacks/early_stopping.py", line 202, in _run_early_stopping_check
    if trainer.fast_dev_run or not self._validate_condition_metric(  # disable early_stopping with fast_dev_run
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/callbacks/early_stopping.py", line 153, in _validate_condition_metric
    raise RuntimeError(error_msg)
RuntimeError: Early stopping conditioned on metric `train/loss_log_MSE` which is not available. Pass in or modify your `EarlyStopping` callback to use any of the following: `train/loss`, `train/acc`, `val/loss`, `val/acc`, `val/auc`
